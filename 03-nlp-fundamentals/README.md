# 03 â€“ NLP Fundamentals ğŸ“

> **The bridge between human language and machine understanding**

---

## ğŸ¯ The Core Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  The Fundamental Problem            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Humans understand: Words, sentences, meaning
Machines understand: Only numbers (0s and 1s)

How do we bridge this gap? â†’ NLP!
```

**Natural Language Processing (NLP)** is the technology that makes GenAI possible.

---

## ğŸŒ‰ NLP: The Bridge

```
Human Language                    Machine Language
     â”‚                                  â”‚
     â”‚  "I love learning AI"            â”‚
     â†“                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Words       â”‚    NLP       â”‚  Numbers     â”‚
â”‚  Sentences   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚  Vectors     â”‚
â”‚  Meaning     â”‚              â”‚  Matrices    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Without NLP, GenAI cannot exist!**

---

## ğŸ“˜ Essential NLP Terminology

### 1ï¸âƒ£ Corpus

**Definition:** A collection of text documents

**Think of it as:** Your entire dataset

**Examples:**

- All tweets from a user
- Complete works of Shakespeare
- Customer reviews for a product
- News articles from 2024

**Code example:**

```python
corpus = """
I love Python. Python is great for AI.
GenAI is transforming the world.
NLP makes GenAI possible.
"""
```

---

### 2ï¸âƒ£ Document

**Definition:** A single unit of text within a corpus

**Typically:**

- One sentence
- One email
- One review
- One article paragraph

**Example breakdown:**

```python
corpus = "I love AI. AI is amazing."

documents = [
    "I love AI.",      # Document 1
    "AI is amazing."   # Document 2
]
```

**Why it matters:** ML models process documents separately, then learn from all of them.

---

### 3ï¸âƒ£ Vocabulary (Vocab)

**Definition:** Set of all **unique words** in your corpus

**Example:**

```
Corpus: "I like apple juice. I like mango juice."

Vocabulary: {I, like, apple, juice, mango}
(5 unique words, even though corpus has 10 words total)
```

**Why important:** Vocabulary size directly affects:

- Model complexity
- Memory requirements
- Training time

**Vocabulary in real models:**

- GPT-3: ~50,000 tokens
- BERT: ~30,000 tokens

---

### 4ï¸âƒ£ Token

**Definition:** The smallest unit of text that the model processes

**Tokens can be:**

- Words â†’ `["Hello", "world"]`
- Subwords â†’ `["un", "##believable"]`
- Characters â†’ `["H", "e", "l", "l", "o"]`

**Modern LLMs use subword tokens** (best of both worlds!)

---

## ğŸ”¹ Tokenization - The Foundation

**Tokenization** = Breaking text into smaller units (tokens)

### Why Tokenization is Critical:

```
âŒ Without Tokenization:
"Hello world" â†’ Cannot process â†’ Stuck!

âœ… With Tokenization:
"Hello world" â†’ ["Hello", "world"] â†’ Can process!
```

---

### Types of Tokenization

#### 1. Sentence Tokenization

**Paragraph â†’ Sentences**

```python
text = "I love AI. It's amazing! NLP is fun."
sentences = ["I love AI.", "It's amazing!", "NLP is fun."]
```

#### 2. Word Tokenization

**Text â†’ Words**

```python
text = "I love NLP"
tokens = ["I", "love", "NLP"]
```

#### 3. Subword Tokenization (Used in GPT, BERT)

**Words â†’ Subwords**

```python
word = "unbelievable"
tokens = ["un", "##believ", "##able"]
```

**Why subwords?**

- Handles rare words âœ…
- Reduces vocabulary size âœ…
- Better generalization âœ…

---

## ğŸ”„ The NLP Pipeline (Simplified)

```
Step 1: Raw Text
    "I LOVE learning GenAI!!!"
    â†“
Step 2: Cleaning
    "i love learning genai"
    â†“
Step 3: Tokenization
    ["i", "love", "learning", "genai"]
    â†“
Step 4: Removing Stopwords
    ["love", "learning", "genai"]
    â†“
Step 5: Stemming/Lemmatization
    ["love", "learn", "genai"]
    â†“
Step 6: Vectorization (Text â†’ Numbers)
    [0.2, 0.8, 0.3, ...] (Vector representation)
    â†“
Step 7: Feed to Model
    ML/DL model can now process!
```

**Each step is covered in detail in Chapter 05!**

---

## â“ Why Tokenization Matters for GenAI

### GenAI Models Don't Read Like Humans

**Humans:**

- Read full sentences
- Understand context instantly
- Process meaning holistically

**GenAI Models:**

- Process one token at a time
- Build context incrementally
- Work with token probabilities

**Example - How ChatGPT generates text:**

```
Prompt: "Write a poem about"

Token 1: "the" (highest probability)
Token 2: "moon" (given "the")
Token 3: "shines" (given "the moon")
Token 4: "bright" (given "the moon shines")
...and so on
```

Each token is predicted based on previous tokens!

---

## ğŸ”¢ From Text to Numbers (Preview)

### The Transformation Journey:

```
Text:    "I love AI"
   â†“
Tokens:  ["I", "love", "AI"]
   â†“
IDs:     [42, 1337, 89] (Token IDs from vocabulary)
   â†“
Vectors: [[0.1, 0.5, 0.2],
          [0.8, 0.3, 0.6],
          [0.4, 0.7, 0.9]]
   â†“
Numbers that models understand! âœ…
```

**This is covered in depth in Chapter 05: ML for NLP**

---

## ğŸ§  Key NLP Concepts (Quick Reference)

| Term             | What It Is              | Example                    |
| ---------------- | ----------------------- | -------------------------- |
| **Corpus**       | Collection of documents | All customer reviews       |
| **Document**     | Single text unit        | One review                 |
| **Vocabulary**   | Unique words            | {good, bad, product, love} |
| **Token**        | Smallest unit           | "love", "AI", "##ing"      |
| **Tokenization** | Splitting text          | "Hello" â†’ ["Hello"]        |
| **Embedding**    | Word as vector          | "king" â†’ [0.2, 0.8, ...]   |

---

## ğŸ¯ Why This Matters for GenAI

### ChatGPT's Secret (Simplified):

1. **Takes your text** â†’ Tokenizes it
2. **Converts to vectors** â†’ Numbers the model understands
3. **Processes through layers** â†’ Neural network magic
4. **Predicts next token** â†’ Probability distribution
5. **Converts back to text** â†’ Human-readable output

**Every step relies on NLP fundamentals!**

---

## ğŸ“Š NLP vs Traditional Programming

| Traditional Code                       | NLP Approach                               |
| -------------------------------------- | ------------------------------------------ |
| `if word == "good": return "positive"` | Learn from 1M examples â†’ predict sentiment |
| Hardcoded rules                        | Learned patterns                           |
| Breaks on new words                    | Handles unseen words                       |
| No context understanding               | Context-aware                              |

---

## âœ… Real-World NLP Applications

| Application            | NLP Task                                   |
| ---------------------- | ------------------------------------------ |
| **ChatGPT**            | Text generation (next token prediction)    |
| **Google Translate**   | Sequence-to-sequence (language â†’ language) |
| **Spam Filter**        | Text classification (spam vs ham)          |
| **Autocomplete**       | Next word prediction                       |
| **Voice Assistants**   | Speech â†’ Text â†’ Understanding â†’ Response   |
| **Sentiment Analysis** | Classify emotion (positive/negative)       |

**All powered by the fundamentals you're learning here!**

---

## ğŸ”‘ Key Takeaways

1. **NLP bridges human language and machine numbers**
2. **Corpus** = all text, **Document** = one text, **Vocabulary** = unique words
3. **Tokenization is mandatory** - Can't process text without it
4. **Tokens become vectors** - That's how machines "understand"
5. **Modern LLMs use subword tokenization** - Best balance

---

## âœ… Self-Check Questions

Before moving to the next chapter:

- [ ] Can you explain why machines can't understand text directly?
- [ ] What's the difference between corpus and document?
- [ ] Can you tokenize a sentence manually?
- [ ] Do you understand why vocabulary size matters?
- [ ] Can you explain tokenization to a friend?

**All yes?** You're ready! ğŸš€

---

## ğŸ¯ What's Next?

You now understand the fundamentals. Time to see the big picture!

**Next Chapter:** [04 - GenAI Overview](../04-genai-overview/README.md)

**What you'll learn:** How all the pieces fit together in GenAI systems

---

## ğŸ’¡ Pro Tip

The concepts here seem simple, but they're **critical**.

When you get to Chapter 05 (ML for NLP), you'll see these concepts in action with real code. Everything will click! ğŸ’¡

---

_Previous: [02 - Python Basics](../02-python-basics/README.md) | Next: [04 - GenAI Overview](../04-genai-overview/README.md)_
