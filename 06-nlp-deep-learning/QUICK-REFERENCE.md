# Quick Reference - NLP in Deep Learning

## ğŸ¯ One-Liner Definitions

| Term                   | Definition                                                     |
| ---------------------- | -------------------------------------------------------------- |
| **Sequential Data**    | Data where order matters (text, time-series)                   |
| **RNN**                | Neural network with feedback loop for sequential data          |
| **BPTT**               | Backpropagation Through Time - training RNNs across time steps |
| **Vanishing Gradient** | Gradients become too small, stopping learning                  |
| **Gradient**           | Rate of change of loss w.r.t. weights                          |
| **LSTM**               | Long Short-Term Memory - RNN variant with gates                |
| **GRU**                | Gated Recurrent Unit - simpler LSTM alternative                |

---

## ğŸ“Š ANN vs RNN

| Feature              | ANN | RNN |
| -------------------- | --- | --- |
| Handles sequence     | âŒ  | âœ…  |
| Memory               | âŒ  | âœ…  |
| NLP tasks            | âŒ  | âœ…  |
| Time-step processing | âŒ  | âœ…  |

---

## ğŸ”‘ Key Formulas

### RNN Forward Pass

```
oâ‚œ = tanh(xâ‚œ Â· Wáµ¢ + oâ‚œâ‚‹â‚ Â· Wâ‚• + b)
```

### Weight Update

```
W_new = W_old âˆ’ Î· Â· (âˆ‚Loss / âˆ‚W_old)
```

---

## ğŸ“ RNN Problems & Solutions

| Problem              | Solution               |
| -------------------- | ---------------------- |
| Vanishing Gradient   | LSTM / GRU             |
| Long-term dependency | LSTM / GRU             |
| Slow training        | GRU (faster than LSTM) |
| Exploding Gradient   | Gradient Clipping      |

---

## ğŸ”„ Learning Roadmap

1. Simple RNN âœ…
2. LSTM
3. GRU
4. Bidirectional RNN
5. Encoder-Decoder
6. Attention Mechanism
7. Transformers
8. LLMs

---

## ğŸ’¡ Interview Quick Points

### Why RNN over ANN for NLP?

- **Order matters in text**
- **ANN loses sequence information**
- **RNN processes word-by-word**
- **RNN has memory (hidden state)**

### Why LSTM over Simple RNN?

- **Solves vanishing gradient**
- **Handles long-term dependencies**
- **Uses gates (forget, input, output)**
- **Better for real-world NLP**

### What is Gradient?

- **Direction to update weights**
- **Rate of change of loss**
- **Used in gradient descent**

---

## ğŸ“‹ RNN Weights

| Weight | Flow            | Purpose         |
| ------ | --------------- | --------------- |
| Wáµ¢     | Input â†’ Hidden  | Process input   |
| Wâ‚•     | Hidden â†’ Hidden | Memory/Context  |
| Wâ‚’     | Hidden â†’ Output | Generate output |

---

## âš¡ Quick Revision

**10-Second Summary:**

- Text is sequential â†’ order matters
- ANN can't handle sequences
- RNN processes step-by-step with memory
- RNN has vanishing gradient problem
- LSTM/GRU solve this

**30-Second Summary:**
Traditional NLP used BoW, TF-IDF, Word2Vec with ML models. But text is sequential - order matters. ANN can't handle this because it processes all words at once and has no memory. RNN solves this by processing words step-by-step and maintaining hidden state. However, simple RNN suffers from vanishing gradient, making it forget long-term context. LSTM and GRU solve this using gate mechanisms.

---

## ğŸ¯ Exam-Perfect Answers

### Q: Why can't ANN handle NLP?

**A:** ANN treats inputs as independent features and processes entire input at once, losing sequence information. In NLP, word order and context are crucial. ANN has no memory mechanism to track previous words, making it unsuitable for language tasks.

### Q: What is vanishing gradient?

**A:** Vanishing gradient occurs when gradients become extremely small during backpropagation through many time steps. This causes earlier time steps to contribute almost nothing to learning, making the model unable to capture long-term dependencies.

### Q: How does RNN maintain context?

**A:** RNN maintains context through hidden state that is passed from one time step to the next. At each step, the hidden state combines current input with previous hidden state, creating a memory of past information.

---

## ğŸ”¢ Types of Data & Models

| Data Type   | Model    | Examples                               |
| ----------- | -------- | -------------------------------------- |
| Tabular     | ANN      | Price prediction, classification       |
| Image/Video | CNN      | Object detection, image classification |
| Sequential  | RNN/LSTM | NLP, time-series, translation          |

---

## ğŸ’» Use Cases

### RNN Applications

- Text generation
- Machine translation
- Sentiment analysis
- Chatbots
- Speech recognition
- Time-series forecasting
- Auto-suggestions

---

## âš ï¸ Common Mistakes to Avoid

1. âŒ Using ANN for sequential data
2. âŒ Ignoring vanishing gradient in simple RNN
3. âŒ Not normalizing gradients (exploding gradient)
4. âŒ Using very deep simple RNN (use LSTM instead)
5. âŒ Forgetting that same weights are reused across time steps

---

## ğŸ¨ Visual Memory Aids

### RNN Flow

```
t1: "The"    â†’ h1 â†’
t2: "food"   â†’ h2 â†’ (remembers "The")
t3: "is"     â†’ h3 â†’ (remembers "The food")
t4: "good"   â†’ h4 â†’ (remembers "The food is")
```

### BPTT Flow

```
Forward: Input â†’ Hidden â†’ Output
Backward: Loss â† Hidden â† All time steps
```

---

**Remember:** This is just the foundation. Real-world NLP uses Transformers (BERT, GPT), which we'll cover later!
